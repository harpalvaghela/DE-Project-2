{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f6309d4-974b-4211-aa4d-51e6a19676b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "##### Date Variable to check new data every day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0853fe1b-b64b-4c96-9392-5dd1216d797f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "todayDate = datetime.today().strftime(\"%Y-%m-%d\")\n",
    " \n",
    "print(f\"Today's Date: {todayDate}\")\n",
    "year, month, day = todayDate.split(\"-\")\n",
    "\n",
    "bronze_base_path = f\"/mnt/project2/bronze/{year}/{month}/{day}/\"\n",
    "silver_base_path = \"/mnt/project2/silver/\"\n",
    "gold_base_path = \"/mnt/project2/gold/\"\n",
    "backup_storage_path = f\"/mnt/project2/client_backup_files/{year}/{month}/{day}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5af14a20-b74d-47fc-b65f-43a2f71393e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Mount Point Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bdc0222-9279-492b-80e6-b3d8186b7a9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "#           \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "#           \"fs.azure.account.oauth2.client.id\": dbutils.secrets.get(scope=\"storage_accounts_scope\",key=\"appid\"),\n",
    "#           \"fs.azure.account.oauth2.client.secret\": dbutils.secrets.get(scope=\"storage_accounts_scope\",key=\"appsecret\"),\n",
    "#           \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/fcee7905-be7c-4a7c-b3f6-7c94700f97cb/oauth2/token\"}\n",
    "\n",
    "# # Optionally, you can add <directory-name> to the source URI of your mount point.\n",
    "# dbutils.fs.mount(\n",
    "#   source = \"abfss://project2@adlsharpalvaghela.dfs.core.windows.net/\",\n",
    "#   mount_point = \"/mnt/project2\",\n",
    "#   extra_configs = configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37a024f1-4ba0-4d4b-8678-be73ce3db5bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Imports all necessory Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aef89a4e-1da0-42bf-961b-82a661cd7abe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType, IntegerType\n",
    "from pyspark.sql.functions import col, when, lit, concat_ws, concat, col, current_timestamp, crc32\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be134e7d-a1cb-434a-9d77-a0fe2404550b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Read Silver Layer Files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70bcd535-80cc-470c-abec-259310c66870",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read Silver Layer Files"
    }
   },
   "outputs": [],
   "source": [
    "def read_silver_layer_files(file_name):\n",
    "    return spark.read.parquet(f\"{silver_base_path}{file_name}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e940470-6249-4315-8aa5-cea9c2e2bfe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Write cleaned file to Silver Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d224166c-3197-4542-9479-784c013b8e52",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Write cleaned file to Silver Layer"
    }
   },
   "outputs": [],
   "source": [
    "def write_single_parquet_file(df, file_name, silver_base_path): \n",
    "    name = file_name.replace(\".csv\", \"\")\n",
    "    temp_path = f\"{silver_base_path}{name}_tmp/\"\n",
    "    final_file_path = f\"{silver_base_path}{name}.parquet\"\n",
    "\n",
    "    # Write to temporary folder as one part file\n",
    "    df.coalesce(1).write.mode(\"overwrite\").parquet(temp_path)\n",
    "\n",
    "    # Find the actual part-xxxx.parquet file\n",
    "    files = dbutils.fs.ls(temp_path)\n",
    "    part_file = [f.path for f in files if f.name.endswith(\".parquet\")][0]\n",
    "\n",
    "    # Move and rename to final single file\n",
    "    dbutils.fs.mv(part_file, final_file_path, True)\n",
    "\n",
    "    # Delete temp folder\n",
    "    dbutils.fs.rm(temp_path, recurse=True)\n",
    "\n",
    "    print(f\"{file_name} saved to silver layer: {final_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49aa76b6-25a6-4b7b-a3db-5680366f40d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Backup files Logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "316ed7a3-f88e-4994-a523-61a2d0ff7b93",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Backup files Logic"
    }
   },
   "outputs": [],
   "source": [
    "from py4j.protocol import Py4JJavaError\n",
    "\n",
    "def path_exists(path):\n",
    "    try:\n",
    "        dbutils.fs.ls(path)\n",
    "        return True\n",
    "    except Py4JJavaError as e:\n",
    "        return \"FileNotFoundException\" not in str(e)\n",
    "\n",
    "def backup_bronze_csv_to_backup_folder(): \n",
    "    if path_exists(bronze_base_path):\n",
    "        for file in dbutils.fs.ls(bronze_base_path):\n",
    "            if file.name.endswith(\".csv\"):\n",
    "                file_name = file.name\n",
    "                entity_name = file_name.replace(\".csv\", \"\")\n",
    "                source_path = bronze_base_path + file_name\n",
    "                temp_target_path = f\"{backup_storage_path}{entity_name}_temp/\"\n",
    "                final_target_path = f\"{backup_storage_path}{file_name}\"\n",
    "\n",
    "                # Read and write as single CSV file to a temp folder\n",
    "                df = spark.read.option(\"header\", True).csv(source_path)\n",
    "                df.coalesce(1).write.option(\"header\", True).mode(\"overwrite\").csv(temp_target_path)\n",
    "\n",
    "                # Rename part-xxxxx.csv to actual filename\n",
    "                part_file = [f.name for f in dbutils.fs.ls(temp_target_path) if f.name.startswith(\"part-\")][0]\n",
    "                dbutils.fs.mv(temp_target_path + part_file, final_target_path)\n",
    "                dbutils.fs.rm(temp_target_path, recurse=True)\n",
    "\n",
    "                print(f\"Backed up {file_name} to {final_target_path}\")\n",
    "    else:\n",
    "        print(f\"No data found in Bronze path: {bronze_base_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eb945cc-7dbd-454b-b1b8-8eb852fafb46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Assign schema for each file in Bronze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aaf49f7-99f5-4788-bdff-42ed16988a4b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Assign schema for each file"
    }
   },
   "outputs": [],
   "source": [
    "# Define Schemas for each file\n",
    "\n",
    "def get_schema(file_name):\n",
    "    if file_name == 'accounts.csv':\n",
    "        return StructType([\n",
    "            StructField(\"account_id\", IntegerType(), True),\n",
    "            StructField(\"customer_id\", IntegerType(), True),    \n",
    "            StructField(\"account_type\", StringType(), True),\n",
    "            StructField(\"balance\", DoubleType(), True)\n",
    "        ])\n",
    "\n",
    "    elif file_name == 'customers.csv':\n",
    "        return StructType([\n",
    "            StructField(\"customer_id\", IntegerType(), True),\n",
    "            StructField(\"first_name\", StringType(), True),\n",
    "            StructField(\"last_name\", StringType(), True),    \n",
    "            StructField(\"address\", StringType(), True),\n",
    "            StructField(\"city\", StringType(), True),\n",
    "            StructField(\"state\", StringType(), True),\n",
    "            StructField(\"zip\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "    elif file_name == 'loans.csv':\n",
    "        return StructType([\n",
    "            StructField(\"loan_id\", IntegerType(), True),\n",
    "            StructField(\"customer_id\", IntegerType(), True),\n",
    "            StructField(\"loan_amount\", DoubleType(), True),\n",
    "            StructField(\"interest_rate\", DoubleType(), True),    \n",
    "            StructField(\"loan_term\", IntegerType(), True)\n",
    "        ])\n",
    "\n",
    "    elif file_name == 'loan_payments.csv':\n",
    "        return StructType([\n",
    "            StructField(\"payment_id\", IntegerType(), True),\n",
    "            StructField(\"loan_id\", IntegerType(), True),\n",
    "            StructField(\"payment_date\", DateType(), True),    \n",
    "            StructField(\"payment_amount\", DoubleType(), True)\n",
    "        ])\n",
    "\n",
    "    elif file_name == 'transactions.csv':\n",
    "        return StructType([\n",
    "            StructField(\"transaction_id\", IntegerType(), True),\n",
    "            StructField(\"account_id\", IntegerType(), True),\n",
    "            StructField(\"transaction_date\", DateType(), True),\n",
    "            StructField(\"transaction_amount\", DoubleType(), True),\n",
    "            StructField(\"transaction_type\", StringType(), True)    \n",
    "        ])\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown file name: {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1a8df2c-2ef2-4933-92eb-a2b857e50c0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Read CSV files in Bronze Folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d9f8754-d5a1-4354-adb6-048f0a15e8a4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read CSV files in Bronze Folder"
    }
   },
   "outputs": [],
   "source": [
    "def list_csv_files_in_bronze(bronze_path): \n",
    "    files = dbutils.fs.ls(bronze_path)\n",
    "    file_names = [file.name for file in files if file.name.endswith(\".csv\")]    \n",
    "    return file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd2d86f3-79d7-4f65-9e9e-fe23a5b45f9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Data Transformation Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "257b6469-410f-4c91-a19f-8d00ecf0a01a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lit\n",
    "\n",
    "#-------------------------------Filter Nulls--------------------------\n",
    "def filter_nulls(df, id_cols):    \n",
    "    for col_name in id_cols:\n",
    "        df = df.filter(col(col_name).isNotNull())\n",
    "    return df\n",
    "\n",
    "#-------------------------------Replace Nulls--------------------------\n",
    "def replace_nulls_with_defaults(df, transform_cols_defaults):\n",
    "    for col_name, default_val in transform_cols_defaults.items():\n",
    "        df = df.withColumn(col_name, when(col(col_name).isNull(), lit(default_val)).otherwise(col(col_name)))\n",
    "    return df\n",
    "\n",
    "#-------------------------------Remove Duplicates--------------------------\n",
    "def remove_duplicates(df, id_cols):   \n",
    "    return df.dropDuplicates(id_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "868a8ba6-416c-4ced-b805-3418362829c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df78a9f8-c861-42e5-afe3-186fb9bf46e4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Drop bank db tables"
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# drop table hive_metastore.bankdb.loan_payments;\n",
    "# drop table hive_metastore.bankdb.loans;\n",
    "# drop table hive_metastore.bankdb.customers;\n",
    "# drop table hive_metastore.bankdb.accounts;\n",
    "# drop table hive_metastore.bankdb.transactions;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c91e5268-5ac1-4ea8-a131-81917e113f56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#spark.sql('DROP DATABASE hive_metastore.bankdb')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1_Mount Point and Functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
