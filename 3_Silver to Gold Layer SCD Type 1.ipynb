{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "063405f4-f2d8-495f-a200-b666b32847b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Silver to Gold Layer\n",
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6400cd10-7c7c-416f-ac54-de0d6f468950",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create database if not exists hive_metastore.bankdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "969eca3a-bb14-4430-95cc-dc23be0113a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS  hive_metastore.bankdb.transactions (\n",
    "    transaction_id INT,\n",
    "    account_id INT,\n",
    "    transaction_date DATE,\n",
    "    transaction_amount FLOAT,\n",
    "    transaction_type STRING,\n",
    "    CreatedDate TIMESTAMP,\n",
    "    UpdatedDate TIMESTAMP,\n",
    "    CreatedBy STRING,\n",
    "    UpdatedBy STRING,\n",
    "    HashKey BIGINT\n",
    ")\n",
    "USING DELTA\n",
    "LOCATION  '/mnt/project2/gold/transactions';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "498f8cf6-bfc5-4d7c-abac-7941c58012cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS hive_metastore.bankdb.loan_payments (\n",
    "    payment_id INT,\n",
    "    loan_id INT,\n",
    "    payment_date DATE,\n",
    "    payment_amount FLOAT,\n",
    "    CreatedDate TIMESTAMP,\n",
    "    UpdatedDate TIMESTAMP,\n",
    "    CreatedBy STRING,\n",
    "    UpdatedBy STRING,\n",
    "    HashKey BIGINT\n",
    ")\n",
    "USING DELTA\n",
    "LOCATION '/mnt/project2/gold/loan_payments';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16f04f40-6b16-478b-9c19-7d4fada1bbe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS hive_metastore.bankdb.loans (\n",
    "    loan_id INT,\n",
    "    customer_id INT,\n",
    "    loan_amount FLOAT,\n",
    "    interest_rate FLOAT,\n",
    "    loan_term INT,\n",
    "    CreatedDate TIMESTAMP,\n",
    "    UpdatedDate TIMESTAMP,\n",
    "    CreatedBy STRING,\n",
    "    UpdatedBy STRING,\n",
    "    HashKey BIGINT\n",
    ")\n",
    "USING DELTA\n",
    "LOCATION '/mnt/project2/gold/loans';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a5fb14f-5a01-4fe0-897c-09af00a41d5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS hive_metastore.bankdb.customers (\n",
    "    customer_id INT,    \n",
    "    first_name STRING,\n",
    "    last_name STRING,\n",
    "    address STRING,\n",
    "    city STRING,\n",
    "    zip STRING,\n",
    "    CreatedDate TIMESTAMP,\n",
    "    UpdatedDate TIMESTAMP,\n",
    "    CreatedBy STRING,\n",
    "    UpdatedBy STRING,\n",
    "    HashKey BIGINT\n",
    ")\n",
    "USING DELTA\n",
    "LOCATION '/mnt/project2/gold/customers';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22a25433-06c3-4327-b1f6-26fe2f41f9f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS hive_metastore.bankdb.accounts (\n",
    "    account_id INT,\n",
    "    customer_id INT,\n",
    "    account_type STRING,\n",
    "    balance DOUBLE,\n",
    "    CreatedDate TIMESTAMP,\n",
    "    UpdatedDate TIMESTAMP,\n",
    "    CreatedBy STRING,\n",
    "    UpdatedBy STRING,\n",
    "    HashKey BIGINT\n",
    ")\n",
    "USING DELTA\n",
    "LOCATION '/mnt/project2/gold/accounts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "774f8ab7-4e15-4c39-a095-857df04d3dd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Check for new Data and load it into the tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8985480f-ef9d-438f-969c-959d2ad286d8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check for new Data and load it into the tables"
    }
   },
   "outputs": [],
   "source": [
    "def scd_type1_logic(\n",
    "    silver_path: str,\n",
    "    gold_path: str,\n",
    "    join_key: str,\n",
    "    update_columns: list,\n",
    "    insert_columns: list,\n",
    "    created_by: str = \"databricks\",\n",
    "    updated_by: str = \"databricks-updated\"\n",
    "):\n",
    "    try:\n",
    "        # Load Silver layer and add hash\n",
    "        df_silver = spark.read.format(\"parquet\").load(silver_path)\n",
    "        from pyspark.sql.functions import current_timestamp, lit\n",
    "        df_hashed = (\n",
    "            df_silver\n",
    "             .withColumn(\"HashKey\", crc32(concat_ws(\"|\", *df_silver.columns)))\n",
    "            .withColumn(\"CreatedDate\", current_timestamp())\n",
    "            .withColumn(\"UpdatedDate\", current_timestamp())\n",
    "            .withColumn(\"CreatedBy\", lit(created_by))\n",
    "            .withColumn(\"UpdatedBy\", lit(created_by))\n",
    "        )\n",
    "      \n",
    "        # Load existing Gold table\n",
    "        target_table = DeltaTable.forPath(spark, gold_path)\n",
    "\n",
    "        # Anti-join to find new/changed rows\n",
    "        df_src = (\n",
    "        df_hashed.alias(\"src\")\n",
    "        .join(\n",
    "            target_table.toDF().alias(\"tgt\"),\n",
    "            col(f\"src.{join_key}\") == col(f\"tgt.{join_key}\"),\n",
    "            \"left\"\n",
    "        )\n",
    "        .filter(\n",
    "            (col(f\"tgt.{join_key}\").isNull()) |  # New records\n",
    "            (col(\"src.HashKey\") != col(\"tgt.HashKey\"))  # Changed records\n",
    "        )\n",
    "        .select(\"src.*\")\n",
    "        )\n",
    "\n",
    "        if df_src.count() > 0:\n",
    "            # Update dictionary\n",
    "            update_set = {colname: f\"source.{colname}\" for colname in update_columns}\n",
    "            update_set[\"UpdatedDate\"] = current_timestamp()\n",
    "            update_set[\"UpdatedBy\"] = lit(updated_by)\n",
    "            update_set[\"HashKey\"] = \"source.HashKey\"\n",
    "\n",
    "            # Insert dictionary\n",
    "            insert_values = {colname: f\"source.{colname}\" for colname in insert_columns}\n",
    "            insert_values.update({\n",
    "                \"CreatedDate\": \"source.CreatedDate\",\n",
    "                \"UpdatedDate\": current_timestamp(),\n",
    "                \"CreatedBy\": \"source.CreatedBy\",\n",
    "                \"UpdatedBy\": lit(created_by),\n",
    "                \"HashKey\": \"source.HashKey\"\n",
    "            })\n",
    "\n",
    "            # Perform merge\n",
    "            target_table.alias(\"target\").merge(\n",
    "                df_src.alias(\"source\"),\n",
    "                f\"target.{join_key} = source.{join_key}\"\n",
    "            ).whenMatchedUpdate(\n",
    "                set = update_set\n",
    "            ).whenNotMatchedInsert(\n",
    "                values = insert_values\n",
    "            ).execute()\n",
    "\n",
    "            print(f\" --> Merge completed for: {gold_path}\")\n",
    "        else:\n",
    "            print(f\"No changes found for: {gold_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in SCD Type 1 process: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cd4e0fe-e450-43a4-b742-f0596e4c2b6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def scd_type1_logic(\n",
    "    silver_path: str,\n",
    "    gold_path: str,\n",
    "    join_key: str,\n",
    "    update_columns: list,\n",
    "    insert_columns: list,\n",
    "    created_by: str = \"databricks\",\n",
    "    updated_by: str = \"databricks-updated\"\n",
    "):\n",
    "    try:\n",
    "        # Load Silver layer and add hash\n",
    "        df_silver = spark.read.format(\"parquet\").load(silver_path)\n",
    "        from pyspark.sql.functions import current_timestamp, lit\n",
    "        df_hashed = (\n",
    "            df_silver\n",
    "             .withColumn(\"HashKey\", crc32(concat_ws(\"|\", *df_silver.columns)))\n",
    "            .withColumn(\"CreatedDate\", current_timestamp())\n",
    "            .withColumn(\"UpdatedDate\", current_timestamp())\n",
    "            .withColumn(\"CreatedBy\", lit(created_by))\n",
    "            .withColumn(\"UpdatedBy\", lit(created_by))\n",
    "        )\n",
    "\n",
    "        # Load existing Gold table\n",
    "        target_table = DeltaTable.forPath(spark, gold_path)\n",
    "\n",
    "        # Anti-join to find new/changed rows\n",
    "        df_src = (\n",
    "        df_hashed.alias(\"src\")\n",
    "        .join(\n",
    "            target_table.toDF().alias(\"tgt\"),\n",
    "            col(f\"src.{join_key}\") == col(f\"tgt.{join_key}\"),\n",
    "            \"left\"\n",
    "        )\n",
    "        .filter(\n",
    "            (col(f\"tgt.{join_key}\").isNull()) |  # New records\n",
    "            (col(\"src.HashKey\") != col(\"tgt.HashKey\"))  # Changed records\n",
    "        )\n",
    "        .select(\"src.*\")\n",
    ")\n",
    "\n",
    "\n",
    "        if df_src.count() > 0:\n",
    "            # Update dictionary\n",
    "            update_set = {colname: f\"source.{colname}\" for colname in update_columns}\n",
    "            update_set[\"UpdatedDate\"] = current_timestamp()\n",
    "            update_set[\"UpdatedBy\"] = lit(updated_by)\n",
    "            update_set[\"HashKey\"] = \"source.HashKey\"\n",
    "\n",
    "            # Insert dictionary\n",
    "            insert_values = {colname: f\"source.{colname}\" for colname in insert_columns}\n",
    "            insert_values.update({\n",
    "                \"CreatedDate\": \"source.CreatedDate\",\n",
    "                \"UpdatedDate\": current_timestamp(),\n",
    "                \"CreatedBy\": \"source.CreatedBy\",\n",
    "                \"UpdatedBy\": lit(created_by),\n",
    "                \"HashKey\": \"source.HashKey\"\n",
    "            })\n",
    "\n",
    "            # Perform merge\n",
    "            target_table.alias(\"target\").merge(\n",
    "                df_src.alias(\"source\"),\n",
    "                f\"target.{join_key} = source.{join_key}\"\n",
    "            ).whenMatchedUpdate(\n",
    "                set = update_set\n",
    "            ).whenNotMatchedInsert(\n",
    "                values = insert_values\n",
    "            ).execute()\n",
    "\n",
    "            print(f\" --> Merge completed for: {gold_path}\")\n",
    "        else:\n",
    "            print(f\"No changes found for: {gold_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in SCD Type 1 process: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0885c525-3c7f-46f8-b9da-61ce3ade9734",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Calling the function"
    }
   },
   "outputs": [],
   "source": [
    "file_infos = dbutils.fs.ls(bronze_base_path)\n",
    "file_names = [f.name for f in file_infos if f.name.endswith(\".csv\")]\n",
    "\n",
    "# Loop through each file\n",
    "for file in file_names:\n",
    "    name = file.replace(\".csv\", \"\")\n",
    "\n",
    "    # Dynamically define join key and columns per file\n",
    "    if name == \"accounts\":\n",
    "        join_key = \"account_id\"\n",
    "        update_cols = insert_cols = [\"account_id\", \"customer_id\", \"account_type\", \"balance\"]\n",
    "\n",
    "    elif name == \"customers\":\n",
    "        join_key = \"customer_id\"\n",
    "        update_cols = insert_cols = [\"customer_id\", \"first_name\",\"last_name\",  \"address\", \"city\", \"zip\"]\n",
    "\n",
    "    elif name == \"loans\":\n",
    "        join_key = \"loan_id\"\n",
    "        update_cols = insert_cols = [\"loan_id\", \"customer_id\", \"loan_amount\", \"interest_rate\", \"loan_term\"]\n",
    "\n",
    "    elif name == \"loan_payments\":\n",
    "        join_key = \"payment_id\"\n",
    "        update_cols = insert_cols = [\"payment_id\", \"loan_id\", \"payment_date\", \"payment_amount\"]\n",
    "\n",
    "    elif name == \"transactions\":\n",
    "        join_key = \"transaction_id\"\n",
    "        update_cols = insert_cols = [\"transaction_id\", \"account_id\", \"transaction_date\", \"transaction_amount\", \"transaction_type\"]\n",
    "\n",
    "    else:\n",
    "        print(f\"Unknown file: {file}\")\n",
    "        continue\n",
    "\n",
    "    silver_path = f\"{silver_base_path}{name}.parquet\"\n",
    "    gold_path = f\"{gold_base_path}{name}\"\n",
    "\n",
    "    print(f\"Processing: {name}\")\n",
    "    scd_type1_logic(\n",
    "        silver_path=silver_path,\n",
    "        gold_path=gold_path,\n",
    "        join_key=join_key,\n",
    "        update_columns=update_cols,\n",
    "        insert_columns=insert_cols\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc17d309-61fa-4819-a296-07a96b486e63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_gold_accounts = spark.read.format(\"delta\").option(\"header\", True).load(f\"/mnt/project2/gold/accounts\")\n",
    "print(\"----------------------------------accounts.csv file Data---------------------------------\")\n",
    "display(df_gold_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87fa0757-750c-4b8b-bd2e-a586cfd52560",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_gold_customers = spark.read.format(\"delta\").option(\"header\", True).load(f\"{gold_base_path}customers\")\n",
    "print(\"---------------------------------customers.csv file Data--------------------------------\")\n",
    "display(df_gold_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2d144e6-f418-4975-b650-b500404949d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_gold_loans = spark.read.format(\"delta\").option(\"header\", True).load(f\"{gold_base_path}loans\")\n",
    "print(\"--------------------------------loans.csv file Data---------------------------------------\")\n",
    "display(df_gold_loans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a225e8e-163e-4c28-8ccd-74038ebac4e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_gold_loan_payments = spark.read.format(\"delta\").option(\"header\", True).load(f\"{gold_base_path}loan_payments\")\n",
    "print(\"-----------------------------loan_payments.csv file Data---------------------------------\")\n",
    "display(df_gold_loan_payments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9017be3-6aad-48af-9610-75498aa10072",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_gold_transactions = spark.read.format(\"delta\").option(\"header\", True).load(f\"{gold_base_path}transactions\")\n",
    "print(\"-------------------------------transactions.csv file Data---------------------------------\")\n",
    "display(df_gold_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f41b10e-8cd7-4070-ba5e-aa5c0244f10d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7435916151829636,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "3_Silver to Gold Layer SCD Type 1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
